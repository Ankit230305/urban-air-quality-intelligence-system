from __future__ import annotations

from pathlib import Path
from datetime import datetime
import numpy as np
import pandas as pd
import streamlit as st
import plotly.express as px

from src.utils.live_fetch import fetch_live_point, livepoint_to_df
from src.utils.clean import (
    coerce_none_like, fill_missing_for_display, drop_empty_columns,
    drop_mostly_empty_columns, has_enough_points, recent_nonnull_window,
    backfill_pm_from_forecast
)
from src.utils.paths import resolve_processed, resolve_forecast_path

st.set_page_config(page_title="Urban AQI", page_icon="ðŸŒ†", layout="wide")
st.title("Urban Air Quality Intelligence System")

DATA_DIR = Path("data")
PROC = DATA_DIR / "processed"
REPORTS = Path("reports")
MODELS = Path("models")

# ---------- helpers ----------
def load_csv_safe(p: Path | str | None, parse_dates=None) -> pd.DataFrame | None:
    try:
        if not p:
            return None
        p = Path(p)
        if not p.exists():
            return None
        return pd.read_csv(p, parse_dates=parse_dates)
    except Exception:
        return None

def ensure_numeric(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:
    df = df.copy()
    for c in cols:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
    return df

def synthetic_processed(city: str) -> pd.DataFrame:
    rng = pd.date_range(end=pd.Timestamp.now().floor("h"), periods=7 * 24, freq="h")
    rs = np.random.RandomState(42)
    return pd.DataFrame(
        {
            "datetime": rng,
            "city": city,
            "pm2_5": np.clip(rs.normal(35, 12, len(rng)), 5, 150),
            "pm10": np.clip(rs.normal(70, 25, len(rng)), 10, 200),
            "no2": np.clip(rs.normal(18, 6, len(rng)), 1, 60),
            "o3": np.clip(rs.normal(25, 10, len(rng)), 1, 120),
            "so2": np.clip(rs.normal(8, 3, len(rng)), 1, 40),
            "co": np.clip(rs.normal(400, 120, len(rng)), 100, 1200),
            "temp": np.clip(rs.normal(298, 4, len(rng)), 285, 310),
            "humidity": np.clip(rs.normal(55, 15, len(rng)), 15, 95),
            "wind_speed": np.clip(rs.normal(2.5, 1, len(rng)), 0, 8),
            "latitude": 0.0,
            "longitude": 0.0,
        }
    )

def _aqi_from_pm25(pm):
    """Compute AQI + category from PM2.5 (Âµg/mÂ³) using EPA breakpoints."""
    if pm is None or (isinstance(pm, float) and np.isnan(pm)):
        return None, "Unknown"
    bp = [
        (0.0, 12.0,     0,  50,   "Good"),
        (12.1, 35.4,   51, 100,   "Moderate"),
        (35.5, 55.4,  101, 150,   "Unhealthy for SG"),
        (55.5, 150.4, 151, 200,   "Unhealthy"),
        (150.5, 250.4,201, 300,   "Very Unhealthy"),
        (250.5, 500.4,301, 500,   "Hazardous"),
    ]
    for c_low, c_high, i_low, i_high, cat in bp:
        if c_low <= pm <= c_high:
            aqi = (i_high - i_low)/(c_high - c_low) * (pm - c_low) + i_low
            return float(round(aqi, 1)), cat
    return 500.0, "Hazardous"

def _sanitize_overview_df(df: pd.DataFrame, slug: str) -> pd.DataFrame:
    """Finalize Overview table:
    - coerce None strings -> NaN
    - backfill pm2_5 from forecast
    - compute AQI + category from pm2_5 when missing
    - drop mostly-empty columns
    """
    if df is None or df.empty:
        return df
    df = coerce_none_like(df)

    # Backfill PM2.5 from forecast
    try:
        fpath_back = resolve_forecast_path(slug)
        _fdf = load_csv_safe(fpath_back, parse_dates=["ds"])
        if _fdf is not None and not _fdf.empty:
            df = backfill_pm_from_forecast(df, _fdf, pm_col="pm2_5")
    except Exception:
        pass

    # AQI & category if missing (dtype-safe)
    if "pm2_5" in df.columns:
        if "aqi" not in df.columns:
            df["aqi"] = np.nan
        if "aqi_category" not in df.columns:
            df["aqi_category"] = np.nan
        miss = df["aqi"].isna()
        if miss.any():
            aqi_vals = pd.Series((_aqi_from_pm25(v)[0] for v in df["pm2_5"]), index=df.index, dtype="float64")
            cat_vals = pd.Series((_aqi_from_pm25(v)[1] for v in df["pm2_5"]), index=df.index, dtype="object")
            df["aqi"] = pd.to_numeric(df["aqi"], errors="coerce")
            df.loc[miss, "aqi"] = aqi_vals[miss]
            df.loc[miss, "aqi_category"] = cat_vals[miss]

    keep = ["datetime","pm2_5","aqi","aqi_category","latitude","longitude","temp","humidity","wind_speed"]
    df = drop_mostly_empty_columns(df, thresh=0.98, keep=keep)
    df = drop_empty_columns(df)
    return df

# ---------- sidebar ----------
with st.sidebar:
    st.header("Controls")
    city = st.text_input("City", value="Hyderabad")
    lat = st.number_input("Latitude", value=17.3850, format="%.6f")
    lon = st.number_input("Longitude", value=78.4867, format="%.6f")
    st.caption("Run the pipeline from terminal to regenerate files.")

tabs = st.tabs(
    ["Overview", "EDA", "Patterns", "Forecasts", "Anomalies", "Health", "Models", "Live Now"]
)
slug = city.lower().replace(" ", "_")

paths = {
    "processed": PROC / f"{slug}_features_plus_demo.csv",
    "anoms": PROC / f"{slug}_anomalies.csv",
    "health": PROC / f"{slug}_health.csv",
    "forecast": MODELS / f"forecast_pm25_{slug}.csv",
    "seasonal": REPORTS / f"seasonal_{slug}.csv",
    "assoc": REPORTS / f"assoc_rules_{slug}.csv",
    "patterns": REPORTS / f"patterns_{slug}.md",
    "live": PROC / f"{slug}__live.csv",
}

# ---------- Overview ----------
with tabs[0]:
    st.subheader("Overview")
    fdf = resolve_processed(city)
    if fdf is None or fdf.empty:
        st.warning("No processed file found. Showing sample data for demo.")
        fdf = synthetic_processed(city)
    fdf = ensure_numeric(
        fdf,
        ["pm2_5","pm10","no2","o3","so2","co","aqi","temp","humidity","wind_speed","latitude","longitude"],
    )
    fdf = _sanitize_overview_df(fdf, slug)

    c1, c2, c3, c4 = st.columns(4)
    disp_aqi = float(fdf["aqi"].dropna().tail(1).mean()) if "aqi" in fdf and fdf["aqi"].notna().any() else np.nan
    c1.metric("AQI", "â€”" if np.isnan(disp_aqi) else f"{disp_aqi:.1f}")
    c2.metric("Rows", f"{len(fdf):,}")
    if "datetime" in fdf.columns:
        c3.metric("Last timestamp", str(pd.to_datetime(fdf["datetime"]).max()))
    c4.metric("Last anomaly", "See Anomalies tab")

    st.dataframe(fdf.tail(300), use_container_width=True)

    st.markdown("### Map")
    try:
        if {"latitude","longitude"}.issubset(fdf.columns) and fdf[["latitude","longitude"]].notna().any().any():
            _mapdf = fdf.dropna(subset=["latitude","longitude"]).tail(1000).copy()
            _mapdf["color"] = _mapdf.get("pm2_5", _mapdf.get("aqi", 0))
            st.plotly_chart(
                px.scatter_mapbox(
                    _mapdf, lat="latitude", lon="longitude", color="color",
                    color_continuous_scale="Turbo", zoom=9, height=360,
                    title="Locations colored by PM2.5/AQI"
                ).update_layout(mapbox_style="open-street-map"),
                use_container_width=True,
            )
        else:
            _mdf = pd.DataFrame({"latitude":[float(lat)],"longitude":[float(lon)], "color":[disp_aqi if not np.isnan(disp_aqi) else 0]})
            st.plotly_chart(
                px.scatter_mapbox(
                    _mdf, lat="latitude", lon="longitude", color="color",
                    color_continuous_scale="Turbo", zoom=10, height=360,
                    title="City marker"
                ).update_layout(mapbox_style="open-street-map"),
                use_container_width=True,
            )
    except Exception:
        st.caption("Map unavailable (missing coords)")

# ---------- EDA ----------
with tabs[1]:
    st.subheader("EDA")
    df = resolve_processed(city)
    if df is None or df.empty:
        st.info("Sample data shown (run pipeline to see real data).")
        df = synthetic_processed(city)
    df = coerce_none_like(df)
    df = ensure_numeric(df, ["pm2_5","pm10","no2","o3","so2","co","temp","humidity","wind_speed","latitude","longitude"])
    # backfill & drop mostly-empty
    try:
        fpath_back = resolve_forecast_path(slug)
        _fdf = load_csv_safe(fpath_back, parse_dates=["ds"])
        if _fdf is not None and not _fdf.empty:
            df = backfill_pm_from_forecast(df, _fdf, pm_col="pm2_5")
    except Exception:
        pass
    df = drop_mostly_empty_columns(df, thresh=0.95)
    df = drop_empty_columns(df)

    if "datetime" in df and "pm2_5" in df and has_enough_points(df, ["datetime","pm2_5"], 10):
        st.plotly_chart(px.line(df, x="datetime", y="pm2_5", title="PM2.5 over time"), use_container_width=True)

    num_cols = [c for c in ["pm2_5","pm10","no2","o3","so2","co"] if c in df.columns]
    if num_cols and has_enough_points(df, num_cols, 10):
        st.plotly_chart(px.histogram(df, x=num_cols, marginal="rug", title="Pollutant distributions", barmode="overlay"),
                        use_container_width=True)

    if {"pm2_5","temp"}.issubset(df.columns) and has_enough_points(df, ["pm2_5","temp"], 10):
        st.plotly_chart(px.scatter(df, x="temp", y="pm2_5", trendline="ols", title="PM2.5 vs Temperature"),
                        use_container_width=True)
    if {"pm2_5","humidity"}.issubset(df.columns) and has_enough_points(df, ["pm2_5","humidity"], 10):
        st.plotly_chart(px.scatter(df, x="humidity", y="pm2_5", trendline="ols", title="PM2.5 vs Humidity"),
                        use_container_width=True)

    if "datetime" in df and "pm2_5" in df and has_enough_points(df, ["datetime","pm2_5"], 24):
        tmp = df[["datetime","pm2_5"]].dropna().copy()
        tmp["weekday"] = pd.to_datetime(tmp["datetime"]).dt.dayofweek
        tmp["hour"] = pd.to_datetime(tmp["datetime"]).dt.hour
        pivot = tmp.pivot_table(index="weekday", columns="hour", values="pm2_5", aggfunc="mean")
        st.plotly_chart(px.imshow(pivot, aspect="auto", title="PM2.5 seasonality (weekday Ã— hour)"),
                        use_container_width=True)

    corr_cols = [c for c in num_cols + ["temp","humidity","wind_speed"] if c in df.columns]
    corr_cols = [c for c in corr_cols if df[c].notna().any()]
    if len(corr_cols) >= 2:
        corr = df[corr_cols].corr(numeric_only=True).fillna(0)
        st.plotly_chart(px.imshow(corr, text_auto=False, title="Correlation heatmap"),
                        use_container_width=True)

# ---------- Patterns ----------
with tabs[2]:
    st.subheader("Seasonality, clustering & association rules")
    seas = load_csv_safe(paths["seasonal"])
    assoc = load_csv_safe(paths["assoc"])
    pat_md = paths["patterns"].read_text() if paths["patterns"].exists() else ""

    if seas is not None and not seas.empty:
        st.markdown("**Daily seasonality (summary)**")
        st.dataframe(drop_empty_columns(seas), use_container_width=True)
    else:
        st.info("No seasonal CSV yet. If processed data exists, run patterns step.")

    if assoc is not None and not assoc.empty:
        st.markdown("**Association rules (top)**")
        assoc = assoc.drop_duplicates()
        for c in ["support","confidence","lift"]:
            if c in assoc.columns:
                assoc[c] = pd.to_numeric(assoc[c], errors="coerce")
        order = [c for c in ["lift","confidence","support"] if c in assoc.columns]
        if order:
            assoc = assoc.sort_values(order, ascending=[False]*len(order))
        st.dataframe(drop_empty_columns(assoc).head(500), use_container_width=True)
    else:
        st.info("No association rules CSV yet.")

    if pat_md:
        st.markdown("**Pattern notes**")
        st.markdown(pat_md)

# ---------- Forecasts ----------
with tabs[3]:
    st.subheader("7-day PM2.5 forecast")
    fpath = resolve_forecast_path(slug)
    fdf = load_csv_safe(fpath, parse_dates=["ds"])
    if fdf is not None and not fdf.empty:
        st.dataframe(fdf.tail(200), use_container_width=True)
        if {"ds","yhat"}.issubset(fdf.columns):
            st.plotly_chart(px.line(fdf, x="ds", y="yhat", title="PM2.5 forecast"),
                            use_container_width=True)
    else:
        st.info("No forecast file yet.")

# ---------- Anomalies ----------
with tabs[4]:
    st.subheader("Anomalies")
    adf = load_csv_safe(paths["anoms"], parse_dates=["datetime"])
    if adf is not None and not adf.empty:
        hide = [c for c in ["temp","humidity","precip","wind_speed","is_anomaly"] if c in adf.columns]
        adf = adf.drop(columns=hide)
        adf = drop_mostly_empty_columns(adf, thresh=0.98)
        st.dataframe(drop_empty_columns(adf).tail(500), use_container_width=True)
        if "score" in adf.columns and "datetime" in adf.columns:
            st.plotly_chart(px.line(adf, x="datetime", y="score", title="Anomaly scores over time"),
                            use_container_width=True)
    else:
        st.info("No anomalies file yet.")

# ---------- Health ----------
with tabs[5]:
    st.subheader("Health risk")
    hdf = load_csv_safe(paths["health"], parse_dates=["datetime"])
    if hdf is not None and not hdf.empty:
        _hide = [c for c in ["temp","humidity","wind_speed","precip"] if c in hdf.columns]
        hdf = hdf.drop(columns=_hide)
        hdf = drop_mostly_empty_columns(hdf, thresh=0.98)
        st.dataframe(drop_empty_columns(hdf).tail(500), use_container_width=True)
        # choose best y
        ycol = "health_risk_score" if "health_risk_score" in hdf.columns else ("risk_index" if "risk_index" in hdf.columns else ("aqi" if "aqi" in hdf.columns else None))
        if ycol:
            zoom = recent_nonnull_window(hdf, ycol, "datetime", days=21)
            st.plotly_chart(px.line(zoom, x="datetime", y=ycol, title=f"Health metric: {ycol} (recent)"),
                            use_container_width=True)
        band = hdf["health_risk_band"].dropna().iloc[-1] if "health_risk_band" in hdf.columns and hdf["health_risk_band"].notna().any() else "Unknown"
        recs = {
            "Good": "Normal activities.",
            "Moderate": "Sensitive groups: limit prolonged exertion.",
            "Unhealthy for SG": "Mask outdoors; limit exertion.",
            "Unhealthy": "Wear N95 outside; reduce outdoor cardio.",
            "Very Unhealthy": "Avoid outdoor activity; purifier indoors.",
            "Hazardous": "Stay indoors; N95 if stepping out.",
            "Unknown": "General caution.",
        }
        st.info(f"Current risk band: **{band}** â€” {recs.get(band, recs['Unknown'])}")
    else:
        st.info("No health file yet.")

# ---------- Models ----------
with tabs[6]:
    st.subheader("Model Scores")
    mfile = MODELS / f"supervised_metrics_{slug}.json"
    if not mfile.exists():
        mfile = MODELS / "supervised_metrics.json"
    if mfile.exists():
        import json
        blob = json.loads(mfile.read_text())
        def _df_from_metrics(d):
            rows = []
            if isinstance(d, dict):
                for model_name, metrics in d.items():
                    if isinstance(metrics, dict):
                        row = {"model": model_name}
                        for k, v in metrics.items():
                            if isinstance(v, (int, float, str)):
                                row[k] = v
                        rows.append(row)
            return pd.DataFrame(rows) if rows else None
        reg_df = _df_from_metrics(blob.get("regression") if isinstance(blob, dict) else None)
        clf_df = _df_from_metrics(blob.get("classification") if isinstance(blob, dict) else None)
        if reg_df is not None and not reg_df.empty:
            st.markdown("**Regression**")
            st.dataframe(reg_df, use_container_width=True)
        if clf_df is not None and not clf_df.empty:
            st.markdown("**Classification**")
            st.dataframe(clf_df, use_container_width=True)
        if (reg_df is None or reg_df.empty) and (clf_df is None or clf_df.empty):
            st.json(blob)
    else:
        st.info("No supervised metrics file found yet. Train models to populate this tab.")

# ---------- Live Now ----------
with tabs[7]:
    st.subheader("Live Now")
    if st.button("Fetch live snapshot"):
        try:
            lp = fetch_live_point(city, float(lat), float(lon))
            df_live = livepoint_to_df(lp)
            st.success(f"Fetched at {lp.fetched_at.isoformat()}")
            m1, m2, m3, m4, m5, m6 = st.columns(6)
            m1.metric("PM2.5 (Âµg/mÂ³)", f"{lp.pm2_5:.1f}")
            m2.metric("PM10 (Âµg/mÂ³)", f"{lp.pm10:.1f}")
            m3.metric("NOâ‚‚ (Âµg/mÂ³)", f"{lp.no2:.1f}")
            m4.metric("Oâ‚ƒ (Âµg/mÂ³)", f"{lp.o3:.1f}")
            m5.metric("SOâ‚‚ (Âµg/mÂ³)", f"{lp.so2:.1f}")
            m6.metric("CO (Âµg/mÂ³)", f"{lp.co:.1f}")
            c1, c2, c3, c4 = st.columns(4)
            c1.metric("Temp (K)", f"{(lp.temp if lp.temp is not None else float('nan')):.1f}")
            c2.metric("Humidity (%)", f"{(lp.humidity if lp.humidity is not None else float('nan')):.0f}")
            c3.metric("Wind (m/s)", f"{(lp.wind_speed if lp.wind_speed is not None else float('nan')):.1f}")
            c4.metric("OWM AQI (1â€“5)", f"{lp.aqi}")
            # Live risk verdict
            live_pm = lp.pm2_5
            aqi, cat = _aqi_from_pm25(live_pm) if live_pm is not None else (lp.aqi, "Unknown")
            verdict_map = {
                "Good": "Safe",
                "Moderate": "Moderate",
                "Unhealthy for SG": "Harmful (sensitive groups)",
                "Unhealthy": "Harmful",
                "Very Unhealthy": "Highly harmful",
                "Hazardous": "Highly harmful",
            }
            verdict = verdict_map.get(cat or "Unknown", "Unknown")
            st.info(f"**Live risk**: {verdict} â€” AQI {aqi if aqi is not None else '-'} ({cat}).")
            st.dataframe(df_live, use_container_width=True)
            if st.checkbox("Append this snapshot to live CSV", value=True):
                paths["live"].parent.mkdir(parents=True, exist_ok=True)
                if paths["live"].exists():
                    base = pd.read_csv(paths["live"])
                    df_live = pd.concat([base, df_live], ignore_index=True)
                df_live.to_csv(paths["live"], index=False)
                st.caption(f"Saved to {paths['live']}")
        except RuntimeError as exc:
            st.error(
                "Live fetch failed.\n\n"
                "Tips:\n"
                "â€¢ Ensure OPENWEATHERMAP_API_KEY is set in .env or your shell.\n"
                "â€¢ Check network connectivity.\n"
                f"â€¢ Details: {exc}"
            )
